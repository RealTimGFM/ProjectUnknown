================================================================================
PROJECT SUMMARY
================================================================================

Generated: 2025-10-13T02:47:20Z
Root: C:\Users\User\Documents\GitHub\ProjectUnknown
Python: 3.13.5

================================================================================
PROJECT TREE
================================================================================

ProjectUnknown/
├── templates/
│   └── index.html
├── uploads/
├── .gitattributes
├── backend.py
├── dump.txt
├── dump_project.py
├── README.md
├── requirements.txt
└── resume_parser.py

Directories: 2  Files: 8

================================================================================
FILE CONTENTS
================================================================================

================================================================================
// Path: README.md
================================================================================

# Mini ATS — Structured Resume Scanner (Phase)

A minimal ATS-style web app:
- Upload a **PDF resume**.
- Backend parses sections and extracts **structured fields** (Experience, Education, Projects).
- Data is stored in **SQLite** with JSON columns.
- Web UI shows **text boxes per field**; you can edit and **save** back to DB.

---

## Features

- Section-aware parsing (SUMMARY / EXPERIENCE / EDUCATION / PROJECTS / SKILLS / LANGUAGES).
- Heuristics for dates and roles; calculates **duration (months)** when possible.
- Stores arrays (lists of dicts) as **JSON** in SQLite.
- Inline **edit & save** for each candidate.
- Safe parser (guards against missing sections / malformed bullets).

---
```powershell
## Project Structure
ProjectUnknown/
├─ backend.py # Flask app + DB CRUD
├─ resume_parser.py # PDF → structured JSON (experience/education/projects)
├─ templates/
│ └─ index.html # Upload + editable cards UI
├─ uploads/ # Saved resumes (created on first upload)
├─ database.db # SQLite (auto-created)
├─ requirements.txt
└─ README.md
## Quickstart
```

> Tested on **Python 3.13**. Works on 3.10+.

### 1) Create venv & install deps
**Windows (PowerShell):**
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt

python backend.py
http://127.0.0.1:5000
# stop the app, then:
del database.db         # PowerShell
# or
rm database.db          # bash/zsh
# start the app again so it recreates the table


================================================================================
// Path: backend.py
================================================================================

# backend.py — Flask upload -> parse -> save -> render
from flask import Flask, request, render_template, jsonify
import sqlite3, os, json, uuid
from resume_parser import parse_resume
from werkzeug.utils import secure_filename

# Optional pure-Python MIME sniff (no system deps). If missing, we just skip.
try:
    import filetype  # pip install filetype
except Exception:
    filetype = None

app = Flask(__name__)
DB_PATH = "database.db"
ALLOWED_EXTS = {"pdf"}
app.config["MAX_CONTENT_LENGTH"] = 10 * 1024 * 1024  # 10 MB


def allowed_file(filename: str) -> bool:
    return "." in filename and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTS


def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
        """CREATE TABLE IF NOT EXISTS candidates (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT,                     -- full display name
            first_name TEXT,
            middle_name TEXT,
            last_name TEXT,
            phone TEXT,
            email TEXT,
            links TEXT,                    -- JSON array of strings
            education TEXT,                -- JSON
            experience TEXT,               -- JSON
            projects TEXT,                 -- JSON
            skills TEXT,                   -- CSV
            languages TEXT,                -- CSV
            raw_text TEXT
        )"""
    )
    conn.commit()
    conn.close()


@app.route("/", methods=["GET", "POST"])
def index():
    if request.method == "POST":
        f = request.files.get("resume")
        if not (f and allowed_file(f.filename)):
            return render_template("index.html", candidates=[], json=json)

        os.makedirs("uploads", exist_ok=True)
        safe = secure_filename(f.filename) or "resume.pdf"
        uid = uuid.uuid4().hex
        path = os.path.join("uploads", f"{uid}_{safe}")
        f.save(path)

        # Optional lightweight MIME check (pip-only; no libmagic)
        if filetype:
            kind = filetype.guess(path)
            if not (kind and kind.mime == "application/pdf"):
                try:
                    os.remove(path)
                except Exception:
                    pass
                return render_template("index.html", candidates=[], json=json)

        parsed = parse_resume(path)

        conn = sqlite3.connect(DB_PATH)
        c = conn.cursor()
        c.execute(
            """INSERT INTO candidates
            (name, first_name, middle_name, last_name, phone, email, links,
                education, experience, skills, languages, raw_text)
            VALUES (?,?,?,?,?,?,?,?,?,?,?,?)""",
            (
                parsed["name"],
                parsed.get("first_name", ""),
                parsed.get("middle_name", ""),
                parsed.get("last_name", ""),
                parsed.get("phone", ""),
                parsed.get("email", ""),
                json.dumps(parsed.get("links", []), ensure_ascii=False),
                json.dumps(parsed.get("education", []), ensure_ascii=False),
                json.dumps(parsed.get("experience", []), ensure_ascii=False),
                parsed.get("skills", ""),
                parsed.get("languages", ""),
                parsed.get("raw_text", ""),
            ),
        )

        conn.commit()
        conn.close()

    # fetch list
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
    "SELECT id, name, first_name, middle_name, last_name, phone, email, links, "
    "education, experience, skills, languages "
    "FROM candidates ORDER BY id DESC"
    )
    rows = c.fetchall()
    conn.close()
    return render_template("index.html", candidates=rows, json=json)


@app.post("/update/<int:cand_id>")
def update_candidate(cand_id: int):
    payload = request.get_json(force=True, silent=True) or {}
    name = payload.get("name", "")
    first_name = payload.get("first_name", "")
    middle_name = payload.get("middle_name", "")
    last_name = payload.get("last_name", "")
    phone = payload.get("phone", "")
    email = payload.get("email", "")
    links = json.dumps(payload.get("links", []), ensure_ascii=False)
    education = json.dumps(payload.get("education", []), ensure_ascii=False)
    experience = json.dumps(payload.get("experience", []), ensure_ascii=False)
    skills = payload.get("skills", "")
    languages = payload.get("languages", "")

    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
    """UPDATE candidates SET
        name=?, first_name=?, middle_name=?, last_name=?, phone=?, email=?, links=?,
        education=?, experience=?, skills=?, languages=? WHERE id=?""",
    (name, first_name, middle_name, last_name, phone, email, links,
    education, experience, skills, languages, cand_id),
    )
    conn.commit()
    conn.close()
    return jsonify({"ok": True})




if __name__ == "__main__":
    init_db()
    app.run(debug=False)


================================================================================
// Path: dump.txt
================================================================================



================================================================================
// Path: dump_project.py
================================================================================

import os
import sys
import argparse
from datetime import datetime
from typing import Iterable, List, Set, Tuple

# ---------- Defaults tuned for StockAI ----------
DEFAULT_ROOT = "."
DEFAULT_OUTPUT = "dump.txt"
DEFAULT_MAX_FILE_SIZE = 1 * 1024 * 1024  # 1 MB

# Folders we usually exclude from **tree** to avoid noise; use --tree-all to include anyway
TREE_EXCLUDE_ANYWHERE: Set[str] = {".git", ".venv", "__pycache__", ".idea", ".vscode"}

# Folders we exclude from **content**, but still show in the tree
CONTENT_EXCLUDE_ANYWHERE: Set[str] = {
    ".git", ".venv", "__pycache__", ".idea", ".vscode", "node_modules",
    "dist", "build", ".mastra",
    # project artifacts
    "backtests", "notebooks",
}

# Filenames to ignore for content
IGNORE_FILES_ANYWHERE: Set[str] = {
    ".env", ".env.local", ".python-version", "Pipfile.lock",
    "package-lock.json", "tsconfig.json", "cookies.txt",
}

# Always include these names (even without extensions) in content
INCLUDE_FILENAMES: Set[str] = {"Makefile", "Dockerfile"}

# Binary-ish extensions (content skipped; still listed in tree)
BINARY_EXTS: Set[str] = {
    # archives / binaries
    ".zip", ".gz", ".tar", ".rar", ".7z", ".exe", ".dll", ".so", ".a",
    # db / parquet / arrow
    ".sqlite", ".sqlite3", ".db", ".db-journal", ".parquet", ".arrow",
    # images
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".tiff", ".ico", ".webp",
    # docs
    ".pdf", ".doc", ".docx", ".ppt", ".pptx", ".xls", ".xlsx",
    # audio/video
    ".mp3", ".wav", ".m4a", ".mp4", ".mov", ".avi", ".mkv",
}

# Text/code extensions included by default for content
DEFAULT_INCLUDE_EXTS: Set[str] = {
    ".py", ".toml", ".yaml", ".yml", ".json", ".md", ".txt", ".cfg", ".ini",
    ".sql", ".sh", ".bat", ".ps1", ".html", ".css", ".js", ".ts", ".tsx", ".jsx",
}
# ------------------------------------------------

def parse_args():
    p = argparse.ArgumentParser(description="Dump project TREE + text/code contents into a single file.")
    p.add_argument("--root", default=DEFAULT_ROOT, help="Root directory to scan")
    p.add_argument("--output", default=DEFAULT_OUTPUT, help="Output file path")
    p.add_argument("--max-size", type=int, default=DEFAULT_MAX_FILE_SIZE, help="Max file size (bytes) for content")
    p.add_argument("--include-ext", default=",".join(sorted(DEFAULT_INCLUDE_EXTS)),
                   help="Comma-separated list of file extensions to include as content (e.g. .py,.toml,.yaml)")
    p.add_argument("--tree-all", action="store_true", help="Include ALL directories in the tree (even .git/.venv)")
    p.add_argument("--no-content", action="store_true", help="Only print the tree (no file contents)")
    p.add_argument("--placeholders", action="store_true",
                   help="Write placeholder blocks for skipped files (binary/large/not-included)")
    return p.parse_args()

def _normalize_exts(s: str) -> Set[str]:
    exts = set()
    for raw in s.split(","):
        e = raw.strip()
        if not e:
            continue
        if not e.startswith("."):
            e = "." + e
        exts.add(e.lower())
    return exts

def is_probably_binary(filepath: str, chunk_size: int = 1024) -> bool:
    try:
        with open(filepath, "rb") as f:
            chunk = f.read(chunk_size)
            if not chunk:
                return False
            if b"\0" in chunk:
                return True
            text_chars = bytearray({7,8,9,10,12,13,27} | set(range(0x20, 0x100)) - {0x7F})
            non_text = sum(1 for b in chunk if b not in text_chars)
            return (non_text / max(1, len(chunk))) > 0.30
    except Exception:
        return True

def should_hide_in_tree(name: str, include_all: bool) -> bool:
    return (name in TREE_EXCLUDE_ANYWHERE) and (not include_all)

def should_skip_content_dir(name: str) -> bool:
    return name in CONTENT_EXCLUDE_ANYWHERE

def should_include_content(fname: str, include_exts: Set[str]) -> bool:
    if fname in INCLUDE_FILENAMES:
        return True
    base, ext = os.path.splitext(fname)
    ext = ext.lower()
    if ext in BINARY_EXTS:
        return False
    if include_exts and ext not in include_exts:
        return False
    return True

def tree_lines(root: str, include_all: bool) -> Tuple[List[str], int, int]:
    """
    Return (lines, dir_count, file_count) for a pretty tree.
    """
    lines: List[str] = []
    dir_count = 0
    file_count = 0

    def walk(dir_path: str, prefix: str = ""):
        nonlocal dir_count, file_count
        try:
            entries = sorted(os.scandir(dir_path), key=lambda e: (not e.is_dir(), e.name.lower()))
        except PermissionError:
            return
        # filter only for tree display
        entries = [e for e in entries if not (e.is_dir() and should_hide_in_tree(e.name, include_all))]
        total = len(entries)
        for idx, e in enumerate(entries):
            connector = "└── " if idx == total - 1 else "├── "
            if e.is_dir():
                lines.append(f"{prefix}{connector}{e.name}/")
                dir_count += 1
                next_prefix = f"{prefix}{'    ' if idx == total - 1 else '│   '}"
                walk(e.path, next_prefix)
            else:
                file_count += 1
                lines.append(f"{prefix}{connector}{e.name}")

    root_label = os.path.basename(os.path.abspath(root)) or root
    lines.append(f"{root_label}/")
    walk(root)
    return lines, dir_count, file_count

def write_header(dump, title: str):
    sep = "=" * max(80, len(title) + 10)
    dump.write(f"{sep}\n")
    dump.write(f"{title}\n")
    dump.write(f"{sep}\n\n")

def write_file_header(dump, rel_path: str):
    header = f"// Path: {rel_path}"
    sep_len = max(80, len(header))
    dump.write("=" * sep_len + "\n")
    dump.write(header + "\n")
    dump.write("=" * sep_len + "\n\n")

def iter_all_files(root: str) -> Iterable[str]:
    for cur_root, dirs, files in os.walk(root, topdown=True):
        dirs[:] = sorted(dirs)
        for fname in sorted(files):
            yield os.path.join(cur_root, fname)

def main():
    args = parse_args()
    root = args.root
    out_path = args.output
    include_exts = _normalize_exts(args.include_ext)

    if not os.path.isdir(root):
        print(f"ERROR: root directory not found: {root}")
        sys.exit(1)

    with open(out_path, "w", encoding="utf-8", errors="ignore") as dump:
        # 1) Project metadata
        write_header(dump, "PROJECT SUMMARY")
        dump.write(f"Generated: {datetime.utcnow().isoformat(timespec='seconds')}Z\n")
        dump.write(f"Root: {os.path.abspath(root)}\n")
        dump.write(f"Python: {sys.version.split()[0]}\n\n")

        # 2) Tree
        write_header(dump, "PROJECT TREE")
        lines, dcnt, fcnt = tree_lines(root, include_all=args.tree_all)
        dump.write("\n".join(lines) + "\n\n")
        dump.write(f"Directories: {dcnt}  Files: {fcnt}\n\n")

        if args.no_content:
            print(f"Done (tree only). Wrote {out_path}")
            return

        # 3) File contents (text/code only)
        write_header(dump, "FILE CONTENTS")
        processed = 0
        skipped = 0

        for fpath in iter_all_files(root):
            rel = os.path.relpath(fpath, root).replace("\\", "/")
            fname = os.path.basename(fpath)
            parent = os.path.basename(os.path.dirname(fpath))

            # Skip content if under excluded dirs
            if parent in CONTENT_EXCLUDE_ANYWHERE or any(p in CONTENT_EXCLUDE_ANYWHERE for p in rel.split("/") if p):
                # still optionally write placeholder
                if args.placeholders:
                    write_file_header(dump, rel)
                    dump.write(f"// [skipped: directory excluded from content]\n\n")
                skipped += 1
                continue

            # Skip content by rule
            if not should_include_content(fname, include_exts):
                if args.placeholders:
                    write_file_header(dump, rel)
                    dump.write(f"// [skipped: extension not in include list]\n\n")
                skipped += 1
                continue

            # Size & binary checks
            try:
                size = os.path.getsize(fpath)
            except OSError as e:
                if args.placeholders:
                    write_file_header(dump, rel)
                    dump.write(f"// [skipped: stat error: {e}]\n\n")
                skipped += 1
                continue

            if size > args.max_size:
                if args.placeholders:
                    write_file_header(dump, rel)
                    dump.write(f"// [skipped: large file ~{size} bytes]\n\n")
                skipped += 1
                continue

            if is_probably_binary(fpath):
                if args.placeholders:
                    write_file_header(dump, rel)
                    dump.write(f"// [skipped: binary-like file]\n\n")
                skipped += 1
                continue

            # Emit content
            try:
                with open(fpath, "r", encoding="utf-8", errors="ignore") as src:
                    content = src.read()
                write_file_header(dump, rel)
                dump.write(content)
                dump.write("\n\n")
                processed += 1
                if processed % 50 == 0:
                    print(f"... wrote {processed} files")
            except Exception as e:
                if args.placeholders:
                    write_file_header(dump, rel)
                    dump.write(f"// [skipped: read error: {e}]\n\n")
                skipped += 1
                continue

        dump.write(f"\n// Summary: wrote {processed} files, skipped {skipped} files.\n")

    print(f"Done. TREE + contents written to {out_path}")

if __name__ == "__main__":
    main()


================================================================================
// Path: requirements.txt
================================================================================

# --- Web (Flask app you have now) ---
Flask>=3.0.3
Werkzeug>=3.0.3
Jinja2>=3.1.4
itsdangerous>=2.2.0
click>=8.1.7

# --- PDF parsing (pip-only) ---
PyMuPDF>=1.24
pdfplumber>=0.11
pdfminer.six>=20231228

# --- OCR fallback (no system deps) ---
easyocr>=1.7.2
torch>=2.3          # CPU wheel

# --- NLP / parsing helpers ---
regex>=2024.9.11
python-dateutil>=2.9.0.post0
phonenumbers>=8.13.44
email-validator>=2.2.0

# --- Optional, pure-Python file type sniffing used in backend.py (safe to keep) ---
filetype>=1.2.0

# --- Nice-to-have logs ---
rich>=13.8


================================================================================
// Path: resume_parser.py
================================================================================

from __future__ import annotations
import re, io
from typing import List, Dict, Tuple
from datetime import datetime

import fitz
import pdfplumber
import easyocr

from dateutil import parser as dateparser
import phonenumbers
from email_validator import validate_email, EmailNotValidError

# ---------- Regexes & helpers ----------
MONTHS = r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*"
YEAR = r"(19|20)\d{2}"
DATE_RE = re.compile(
    rf"(?P<start>({MONTHS}\s+{YEAR}|{YEAR}))\s*[-–—]\s*(?P<end>({MONTHS}\s+{YEAR}|{YEAR}|Present|Current))",
    re.IGNORECASE,
)

EDU_LEVEL_RE = re.compile(
    r"(High School Diploma|Diploma(?: of College Studies)?(?:\s*DEC)?|Bachelor(?:'s)?|Master(?:'s)?|BSc|MSc|PhD)",
    re.I,
)
EDU_SCHOOL_HINT = re.compile(r"(University|College|School|Institute)", re.I)
LOCATION_LINE_RE = re.compile(r"[A-Z][a-z]+(?:,?\s+[A-Z][a-z]+)+(?:,\s*[A-Za-z.]+)?")

SCHOOL_LOC_SPLIT_RE = re.compile(
    r"^(?P<school>.*?(?:University|College|School|Institute))\s*[,–—-]\s*(?P<loc>.+)$",
    re.I,
)

CONTACT_EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
CONTACT_PHONE_RE = re.compile(r"(\+?\d[\d\s().-]{7,}\d)")
URL_RE = re.compile(r"(https?://[^\s]+|(?:www\.)?(?:github|linkedin)\.com[^\s]*)", re.I)
LOCATION_HINTS = ["Montreal", "QC", "Canada", "Ho Chi Minh", "Viet", "Vietnam"]
# --- role & company heuristics for experience titles ---
ROLE_WORDS = {
    "intern",
    "developer",
    "engineer",
    "analyst",
    "manager",
    "consultant",
    "architect",
    "administrator",
    "specialist",
    "scientist",
    "lead",
    "principal",
    "director",
    "founder",
    "co-founder",
    "sre",
    "devops",
    "tester",
    "qa",
    "designer",
}
COMPANY_SUFFIX = re.compile(
    r"\b(inc\.?|corp\.?|llc|ltd\.?|co\.?|company|capital|fund|bank|group|partners?|"
    r"systems?|labs?|studio|technolog(?:y|ies)|solutions?)\b",
    re.I,
)
TITLE_HINT = re.compile(
    r"\b(senior|sr\.?|jr\.?|junior|lead|principal|staff|head|director|manager|"
    r"engineer|developer|analyst|consultant|architect|intern)\b",
    re.I,
)
BULLET_LINE = re.compile(r"^\s*[-*•]\s+")


def _looks_like_title(s: str) -> bool:
    s = _norm(s)
    if not s or BULLET_LINE.match(s):
        return False
    if s.endswith("."):  # full sentence → likely not a title
        return False
    if TITLE_HINT.search(s):  # contains role keywords
        return True
    # title-case heuristic: many tokens start uppercase but not all caps
    toks = [t for t in s.split() if t.isalpha()]
    if not toks:
        return False
    caps = sum(1 for t in toks if t[0].isupper() and not t.isupper())
    return caps / len(toks) >= 0.6 and len(toks) <= 7


def _looks_like_company(s: str) -> bool:
    s = _norm(s)
    if not s or BULLET_LINE.match(s):
        return False
    if s.lower().startswith(("http://", "https://", "www.")):
        return False
    if COMPANY_SUFFIX.search(s):
        return True
    # many company lines are all Title Case but short
    toks = [t for t in s.split() if t.isalpha()]
    caps = sum(1 for t in toks if t[0].isupper())
    return caps >= 2 and len(toks) <= 8


def _guess_title_company_from_buffer(buf: list[str]) -> tuple[str, str]:
    """
    Look back over the last few non-empty lines and pick a company & title.
    Prefer: company line with suffixes (Inc., Corp.) and the line just above it as title.
    Fallback: any title-like line, then any company-like line.
    """
    cand_title, cand_company = "", ""
    window = [_norm(x) for x in buf if _norm(x)][-6:]  # last ~6 non-empty lines
    # pass 1: company first then title just above
    for i in range(len(window) - 1, -1, -1):
        if _looks_like_company(window[i]):
            cand_company = window[i]
            # title likely the nearest line above
            for j in range(i - 1, -1, -1):
                if _looks_like_title(window[j]):
                    cand_title = window[j]
                    break
            break
    # pass 2: if title still empty, find any title-like line
    if not cand_title:
        for i in range(len(window) - 1, -1, -1):
            if _looks_like_title(window[i]):
                cand_title = window[i]
                break
    # pass 3: if company still empty, take any company-like line
    if not cand_company:
        for i in range(len(window) - 1, -1, -1):
            if _looks_like_company(window[i]):
                cand_company = window[i]
                break
    return cand_title, cand_company


# --- add near the top (after imports/regex) ---
# Header labels that appear in the SKILLS block; we'll strip these off lines
CATEGORY_PREFIX = re.compile(
    r"""^\s*(
        languages?|
        frameworks(?:\s*/\s*|\s*&\s*)?libraries|
        frameworks|libraries|
        data\s*/?\s*ai|data/ai|data|ai|
        databases?|
        infra(?:structure)?(?:\s*&\s*hosting)?|infra\s*&\s*hosting|hosting|
        tooling|tools|
        technology\s*stack|tech\s*stack|stack
    )\s*:\s*""",
    re.I | re.X,
)

# Multi-word phrases to capture as a single skill BEFORE generic tokenizing
MULTI_PHRASES = [
    "semantic search",
    "cosine similarity",
    "visual studio 2022",
    "vs code",
    "tailwind css",
    "sql server",
    "azure vm",
    "jakarta ee",
    "better-sqlite3",
]

# Canonical casing for common skills/tools
CANON = {
    "c#": "C#",
    "c++": "C++",
    ".net": ".NET",
    "asp.net": "ASP.NET",
    "javascript": "JavaScript",
    "typescript": "TypeScript",
    "numpy": "NumPy",
    "sbert": "SBERT",
    "t-sql": "T-SQL",
    "sql server": "SQL Server",
    "ssms": "SSMS",
    "mysql": "MySQL",
    "phpmyadmin": "phpMyAdmin",
    "sqlite": "SQLite",
    "h2": "H2",
    "azure vm": "Azure VM",
    "iaas": "IaaS",
    "iis": "IIS",
    "tomcat": "Tomcat",
    "git": "Git",
    "github": "GitHub",
    "visual studio 2022": "Visual Studio 2022",
    "vs code": "VS Code",
    "tailwind css": "Tailwind CSS",
    "jakarta ee": "Jakarta EE",
    "flask": "Flask",
    "pandas": "pandas",
    "scikit-learn": "scikit-learn",
}

# Tokens to ignore if they accidentally slip through (noise/fillers)
STOPWORDS = re.compile(
    r"^(incl|including|and|with|using|basic|intermediate|advanced|expert|proficient|the|a|an|data|ai)$",
    re.I,
)

SKILL_PHRASES = {
    # technical (add freely)
    "python",
    "java",
    "javascript",
    "typescript",
    "c",
    "c++",
    "c#",
    ".net",
    "node.js",
    "react",
    "vue",
    "angular",
    "html",
    "css",
    "sass",
    "sql",
    "postgresql",
    "mysql",
    "sqlite",
    "mongodb",
    "nosql",
    "redis",
    "aws",
    "azure",
    "gcp",
    "docker",
    "kubernetes",
    "linux",
    "git",
    "github",
    "gitlab",
    "ci/cd",
    "terraform",
    "ansible",
    "spark",
    "hadoop",
    "pandas",
    "numpy",
    "scikit-learn",
    "opencv",
    "pytorch",
    "tensorflow",
    "fastapi",
    "flask",
    "django",
    "rest",
    "grpc",
    "graphql",
    "rabbitmq",
    "kafka",
    "elasticsearch",
    # soft/general
    "agile",
    "scrum",
    "kanban",
    "communication",
    "leadership",
    "mentoring",
    "problem solving",
    "teamwork",
}
TECH_TOKEN = re.compile(
    r"""
    (?:                             # known multi-char tokens first (verbose mode)
        C\+\+ | C\# | \.NET | Node\.js | React | Vue | Angular | CI/CD | SQL | NoSQL
    )
    |
    (?:[A-Za-z][\w.+#-]*)           # generic tech-ish token (allows ., +, #, -)
    """,
    re.I | re.X,
)


def _extract_skills(sections: dict, experience: list) -> str:
    """
    RAW mode: return EVERYTHING inside the SKILLS section exactly as text.
    We only trim leading/trailing whitespace per line and join lines with spaces.
    No splitting, no aliasing, no dedupe, no tokenization.
    """
    lines = sections.get("SKILLS", []) or []
    # keep original order; preserve punctuation/colons/parentheses/etc.
    cleaned = [(ln or "").strip() for ln in lines if (ln or "").strip()]
    return " ".join(cleaned)  # single-line for the UI input


def _norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()


def _duration_months_inclusive(start_dt: datetime, end_dt: datetime) -> int:
    return max(
        0, (end_dt.year - start_dt.year) * 12 + (end_dt.month - start_dt.month) + 1
    )


# ---------- PDF text extraction with OCR fallback ----------
_reader = None  # lazy EasyOCR reader (global to reuse weights)


def _get_ocr_reader():
    global _reader
    if _reader is None:
        # English by default; add more codes like ['en','fr'] if needed
        _reader = easyocr.Reader(["en"], gpu=False)
    return _reader


def _read_pdf_text(path: str) -> str:
    """
    1) Try PyMuPDF text. If a page has too little text, render page -> image and OCR with EasyOCR.
    2) Optionally blend pdfplumber text as a hint if both are sparse.
    """
    doc = fitz.open(path)
    parts: List[str] = []
    sparse = 0
    for page in doc:
        t = page.get_text("text") or ""
        if len(t.strip()) < 50:
            sparse += 1
            # OCR fallback
            pix = page.get_pixmap(dpi=300, alpha=False)
            img_bytes = pix.tobytes("png")
            reader = _get_ocr_reader()
            result = reader.readtext(img_bytes, detail=0, paragraph=True)
            t = "\n".join(result)
        parts.append(t)
    doc.close()

    text = "\n".join(parts).strip()

    # As an extra hint, try pdfplumber if we still got almost nothing
    if len(text) < 80:
        try:
            with pdfplumber.open(path) as pdf:
                more = "\n".join((p.extract_text() or "") for p in pdf.pages)
            if len(more.strip()) > len(text):
                text = more
        except Exception:
            pass

    return (text or "") + "\n"


# ---------- Section splitting ----------
def _split_sections(text: str) -> dict:
    """
    Split resume text into sections using tolerant regex headers.
    If we're already inside SKILLS and we see subcategory lines like
    'Languages: ...' or 'Frameworks/Libraries: ...', keep them as SKILLS content
    instead of starting a new section.
    """
    sections, current = {}, None

    header_patterns = {
        "SUMMARY": re.compile(
            r"^(summary|professional summary|profile)\b[:\-–—]?", re.I
        ),
        "EXPERIENCE": re.compile(
            r"^(experience|work experience|employment|professional experience)\b[:\-–—]?",
            re.I,
        ),
        "EDUCATION": re.compile(r"^(education|academic background)\b[:\-–—]?", re.I),
        # treat many synonyms as Skills headers (top-level)
        "SKILLS": re.compile(
            r"^(skills|technical skills|tech skills|skills & interests|skills and interests|"
            r"core competencies|competencies|technical proficienc(?:y|ies)|proficienc(?:y|ies)|"
            r"technolog(?:y|ies)|tools|tooling|tech stack|technology stack|stack|"
            r"programming (?:skills|languages)|frameworks(?:\s*/\s*|\s*&\s*)?libraries|frameworks|libraries|strengths)"
            r"\b[:\-–—]?",
            re.I,
        ),
        # real separate LANGUAGES section (e.g., human languages)
        "LANGUAGES": re.compile(r"^languages\b[:\-–—]?", re.I),
        # kept for completeness
        "PROJECTS": re.compile(
            r"^(projects|personal projects|notable projects)\b[:\-–—]?", re.I
        ),
    }

    def has_inline_content_after_colon(line: str) -> bool:
        i = line.find(":")
        return i != -1 and line[i + 1 :].strip() != ""

    for raw in text.splitlines():
        line = _norm(raw)
        if not line:
            continue

        matched_header = None
        for key, pat in header_patterns.items():
            if pat.match(line):
                matched_header = key
                break

        if matched_header:
            # --- IMPORTANT RULE ---
            # If we're already in SKILLS and this "header" has inline content (e.g., "Languages: C#, Java"),
            # treat it as a SKILLS content row (subcategory) instead of starting a new section.
            if current == "SKILLS" and has_inline_content_after_colon(line):
                sections.setdefault("SKILLS", []).append(line)
                continue

            # If this is the very first SKILLS-like header *and* it carries inline content on the same line,
            # start SKILLS and keep that line as content too.
            if matched_header == "SKILLS" and has_inline_content_after_colon(line):
                current = "SKILLS"
                sections.setdefault("SKILLS", [])
                sections["SKILLS"].append(line)
                continue

            # Otherwise, switch sections normally.
            current = matched_header
            sections.setdefault(current, [])
            continue

        if current:
            sections[current].append(line)

    return sections


def _parse_date_range(s: str) -> Tuple[str | None, str | None, int | None]:
    m = DATE_RE.search(s or "")
    if not m:
        return None, None, None
    start_raw, end_raw = m.group("start"), m.group("end")
    try:
        start_dt = dateparser.parse(start_raw, fuzzy=True)
    except Exception:
        start_dt = None
    if end_raw and re.search(r"present|current", end_raw or "", re.I):
        end_dt = datetime.utcnow()
    else:
        try:
            end_dt = dateparser.parse(end_raw, fuzzy=True) if end_raw else None
        except Exception:
            end_dt = None
    duration = None
    if start_dt and end_dt:
        duration = _duration_months_inclusive(start_dt, end_dt)
    return start_raw, end_raw, duration


def _extract_contacts(text: str) -> Dict[str, str | list]:
    # phone
    phone = ""
    m = CONTACT_PHONE_RE.search(text or "")
    if m:
        try:
            pn = phonenumbers.parse(m.group(0), "CA")
            phone = (
                phonenumbers.format_number(
                    pn, phonenumbers.PhoneNumberFormat.INTERNATIONAL
                )
                if phonenumbers.is_valid_number(pn)
                else _norm(m.group(0))
            )
        except Exception:
            phone = _norm(m.group(0))
    # email
    email = ""
    m = CONTACT_EMAIL_RE.search(text or "")
    if m:
        try:
            email = validate_email(m.group(0)).normalized
        except EmailNotValidError:
            email = _norm(m.group(0))
    # urls
    urls = [_norm(u) for u in URL_RE.findall(text or "")]
    # rough location line
    loc = ""
    for line in (text or "").splitlines():
        if any(h in line for h in LOCATION_HINTS) or re.search(
            r"[A-Z][a-z]+(?:,?\s+[A-Z][a-z]+)+(?:,\s*[A-Za-z]+)?", line
        ):
            loc = _norm(line)
            break
    return {"phone": phone, "urls": urls, "email": email, "location": loc}


def _guess_name(lines: List[str]) -> str:
    # choose the first plausible name-like line (titlecased tokens, not contact)
    for raw in lines[:20]:
        s = _norm(raw)
        if not s:
            continue
        if CONTACT_EMAIL_RE.search(s) or CONTACT_PHONE_RE.search(s) or URL_RE.search(s):
            continue
        tokens = [t for t in s.split() if re.match(r"^[A-Z][a-zA-Z-]+$", t)]
        if len(tokens) >= 2:
            return s
    return next((_norm(l) for l in lines if _norm(l)), "Unknown")


# ---------- Section parsers ----------
def _parse_experience(lines: List[str]) -> List[Dict]:
    items: List[Dict] = []
    cur: Dict | None = None
    prebuf: List[str] = []  # recent non-empty lines before the date line
    company_hint = COMPANY_SUFFIX  # reuse pattern

    def start_new_job(date_line: str):
        nonlocal cur
        start, end, dur = _parse_date_range(date_line)
        head = DATE_RE.split(date_line)[0].strip(" -–—|")
        parts = [p.strip() for p in re.split(r"[|•\-–—]", head) if p.strip()]
        # Try to get title/company from the 'head' (left of dates) if present
        head_title = parts[0] if parts else ""
        head_company = parts[1] if len(parts) >= 2 else ""
        # If head is empty (common in two-column resumes), backscan prebuf
        buf_title, buf_company = _guess_title_company_from_buffer(prebuf)

        title = _norm(head_title or buf_title)
        company = _norm(head_company or buf_company)

        cur = {
            "position": title,
            "company_name": company,
            "location": "",
            "start_date": start,
            "end_date": end,
            "duration_months": dur,
            "description": "",
            "skills_used_tech": [],
            "skills_used_soft": [],
        }

    for ln in lines:
        line = _norm(ln)
        if not line:
            continue

        # New job boundary: a line with a date range
        if DATE_RE.search(line):
            if cur:
                # close previous job
                items.append(cur)
            start_new_job(line)
            prebuf.clear()
            continue

        # Accumulate pre-buffer for title/company discovery before date line
        if not cur:
            prebuf.append(line)
            # keep last ~6 meaningful lines
            if len(prebuf) > 8:
                prebuf = prebuf[-8:]
            continue

        # Within a job block → grow description & fill missing metadata
        if BULLET_LINE.match(line):
            clean = BULLET_LINE.sub("", line).strip()
        else:
            clean = line
        cur["description"] = (
            cur["description"] + ("\n" if cur["description"] else "") + clean
        ).strip()

        # Fill company/location from body hints if still missing
        if not cur["company_name"] and company_hint.search(line):
            cur["company_name"] = line
        if not cur["location"]:
            if any(h.lower() in line.lower() for h in LOCATION_HINTS):
                cur["location"] = line

        # Sometimes a title appears again near the top of bullet block (e.g., "Developer")
        if not cur["position"] and _looks_like_title(line):
            cur["position"] = line

    if cur:
        items.append(cur)

    # Post-fixes
    for e in items:
        if not e["company_name"] and re.search(
            r"Machine Builder Inc\.?", e.get("description", ""), re.I
        ):
            e["company_name"] = "Machine Builder Inc."
        if not e["location"] and re.search(r"Montreal", e.get("description", ""), re.I):
            e["location"] = "Montreal"
        if e.get("start_date") and (
            not e.get("end_date")
            or re.search(r"present|current", e.get("end_date") or "", re.I)
        ):
            # recompute inclusive months to now
            _, _, e["duration_months"] = _parse_date_range(
                f"{e['start_date']} - Present"
            )

    return items


def _parse_education(lines: List[str]) -> List[Dict]:
    """
    Each education entry starts at a 'level' line (e.g., 'High School Diploma',
    'Diploma of College Studies DEC – Computer Science', 'Bachelor ...', etc).
    We then attach up to the next 2 relevant lines (school and/or location).
    """
    items: List[Dict] = []
    i, n = 0, len(lines)

    def parse_block(buf: List[str]) -> Dict:
        # Join for date parsing, but keep lines to pull school/location reliably
        text = " | ".join(buf)

        # Dates anywhere in the small block
        start, end, _ = _parse_date_range(text)

        # Level & Field
        level = ""
        field = ""
        m = EDU_LEVEL_RE.search(buf[0])
        if m:
            level = _norm(m.group(1))
        # DEC fields often after a dash on the same first line
        m_dec = re.search(r"DEC\s*[–—-]\s*([A-Za-z &/+\-]{2,})", buf[0], re.I)
        if m_dec:
            field = _norm(m_dec.group(1))
        elif re.search(r"(Bachelor|Master|BSc|MSc|PhD)", buf[0], re.I):
            m_field = re.search(r"(?:in|of)\s+([A-Za-z &/+\-]{2,})", buf[0], re.I)
            if m_field:
                field = _norm(m_field.group(1))

        # School & Location
        school, location = "", ""

        # 1) If the first line is "Level – School", split that
        parts = [p.strip() for p in re.split(r"[–—-]", buf[0]) if p.strip()]
        if len(parts) >= 2 and EDU_SCHOOL_HINT.search(parts[-1]):
            school = _norm(parts[-1])

        # 2) Otherwise, look at following lines for school/location
        for s in buf[1:]:
            s_norm = _norm(s)
            # line like "LaSalle College, Montreal, QC"
            msl = SCHOOL_LOC_SPLIT_RE.match(s_norm)
            if msl:
                school = school or _norm(msl.group("school"))
                location = _norm(msl.group("loc"))
                continue
            # pure school line
            if EDU_SCHOOL_HINT.search(s_norm):
                school = school or s_norm
                continue
            # pure location line
            if LOCATION_LINE_RE.search(s_norm):
                location = location or s_norm
                continue

        # Canonicals for your examples (optional)
        if re.search(r"High School Diploma", text, re.I) and re.search(
            r"Lawrence", text, re.I
        ):
            school = "Lawrence S. Ting School"
        if re.search(r"\bDEC\b", text, re.I) and re.search(r"LaSalle", text, re.I):
            school = "LaSalle College"

        return {
            "level": level,
            "field": field,
            "school_name": school,
            "location": location,
            "start_year": start,
            "end_year": end,
        }

    # Walk the EDUCATION lines; a level line starts a new entry
    while i < n:
        line = _norm(lines[i])
        i += 1
        if not line:
            continue

        if EDU_LEVEL_RE.search(line):
            # Start a new entry and attach up to next TWO relevant lines
            buf = [line]
            attach = 0
            while i < n and attach < 2:
                nxt = _norm(lines[i])
                # Stop when next 'level' begins or another section header appears
                if not nxt:
                    i += 1
                    continue
                if EDU_LEVEL_RE.search(nxt):
                    break
                if re.match(
                    r"^(experience|projects|skills|summary|languages)\b", nxt, re.I
                ):
                    break
                buf.append(nxt)
                attach += 1
                i += 1
            items.append(parse_block(buf))

    # Keep only non-empty entries
    return [
        e
        for e in items
        if any([e["level"], e["school_name"], e["start_year"], e["end_year"]])
    ]


def _parse_projects(lines: List[str]) -> List[Dict]:
    items, buf = [], []

    def dates_in(s: str) -> str:
        m = DATE_RE.search(s or "")
        return m.group(0) if m else ""

    def clean_title(s: str) -> str:
        s = re.sub(DATE_RE, "", s or "")
        s = re.sub(r"\b(20\d{2}|19\d{2})\b", "", s)
        return _norm(s)

    def flush():
        if not buf:
            return
        text = "\n".join(buf).strip()
        title_line = buf[0] if buf else ""
        items.append(
            {
                "title": clean_title(title_line),
                "when": dates_in(" ".join(buf)),
                "description": text,
            }
        )
        buf.clear()

    for ln in lines:
        if not (ln or "").startswith(("•", "-", "*")) and buf:
            flush()
        buf.append(ln or "")
    flush()
    return items


def _split_name(full: str) -> tuple[str, str, str]:
    full = _norm(full)
    if not full or full == "Unknown":
        return "", "", ""
    parts = full.split()
    if len(parts) == 1:
        return parts[0], "", ""
    if len(parts) == 2:
        return parts[0], "", parts[1]
    return parts[0], " ".join(parts[1:-1]), parts[-1]


def parse_resume(filepath: str) -> Dict:
    text = _read_pdf_text(filepath)
    lines = [l for l in (text or "").splitlines()]
    full_name = _guess_name(lines)
    first, middle, last = _split_name(full_name)

    sections = _split_sections(text)
    contacts = _extract_contacts(text)

    education = _parse_education(sections.get("EDUCATION", []))
    experience = _parse_experience(sections.get("EXPERIENCE", []))

    skills_csv = _extract_skills(sections, experience)

    return {
        "name": _norm(" ".join([p for p in [first, middle, last] if p])),
        "first_name": first,
        "middle_name": middle,
        "last_name": last,
        "phone": contacts.get("phone", ""),
        "email": contacts.get("email", ""),
        "links": contacts.get("urls", []),
        "education": education,
        "experience": experience,
        # projects intentionally removed from payload
        "skills": skills_csv,
        "languages": "",
        "raw_text": text or "",
    }


================================================================================
// Path: templates/index.html
================================================================================

<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>ATS Scanner</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <style>
    :root {
      --bg: #0b0f1a;
      --card: #0f1422;
      --muted: #9aa3af;
      --accent: #3b82f6;
      --border: #1f2937;
      --text: #e5e7eb;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
      background: var(--bg);
      color: var(--text);
    }

    header {
      padding: 20px;
      background: #0a0f1d;
      border-bottom: 1px solid var(--border);
    }

    header h1 {
      margin: 0;
      font-size: 20px;
    }

    main {
      max-width: 1100px;
      margin: 20px auto;
      padding: 0 16px;
    }

    .upload {
      background: var(--card);
      padding: 16px;
      border-radius: 12px;
      border: 1px solid var(--border);
      display: flex;
      gap: 12px;
      align-items: center;
    }

    .upload input[type=file] {
      flex: 1;
      color: var(--muted);
    }

    .upload button {
      background: var(--accent);
      color: #fff;
      border: none;
      padding: 10px 14px;
      border-radius: 8px;
      cursor: pointer;
    }

    .card {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 14px;
      padding: 18px;
      margin: 16px 0;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.12);
    }

    .grid {
      display: grid;
      grid-template-columns: repeat(12, 1fr);
      gap: 12px;
    }

    .col-12 {
      grid-column: span 12;
    }

    .col-6 {
      grid-column: span 6;
    }

    .col-4 {
      grid-column: span 4;
    }

    .col-3 {
      grid-column: span 3;
    }

    label {
      font-size: 12px;
      color: var(--muted);
      display: block;
      margin-bottom: 6px;
    }

    input,
    textarea {
      width: 100%;
      padding: 10px;
      border: 1px solid var(--border);
      border-radius: 10px;
      background: #0b1120;
      color: var(--text);
    }

    textarea {
      min-height: 80px;
      resize: vertical;
    }

    .row {
      display: flex;
      gap: 10px;
      align-items: center;
    }

    .badge {
      display: inline-block;
      padding: 4px 10px;
      background: #1f2a44;
      color: #93c5fd;
      border-radius: 999px;
      font-size: 12px;
      margin-right: 6px;
      border: 1px solid var(--border);
    }

    .btn {
      border: 1px solid var(--border);
      background: #0b1120;
      color: var(--text);
      padding: 8px 12px;
      border-radius: 8px;
      cursor: pointer;
    }

    .btn.primary {
      background: var(--accent);
      color: #fff;
      border-color: var(--accent);
    }

    .section {
      margin-top: 16px;
      padding-top: 16px;
      border-top: 1px dashed var(--border);
    }

    .muted {
      color: var(--muted);
    }
  </style>
</head>

<body>
  <header>
    <h1>ATS Scanner — Structured Resume Parser</h1>
  </header>
  <main>
    <form class="upload" method="POST" enctype="multipart/form-data">
      <input type="file" name="resume" accept=".pdf" required>
      <button type="submit">Upload & Parse</button>
    </form>
    {% for c in candidates %}
    {% set cid = c[0] %}
    {% set name = c[1] %}
    {% set first = c[2] or "" %}
    {% set middle = c[3] or "" %}
    {% set last = c[4] or "" %}
    {% set phone = c[5] or "" %}
    {% set email = c[6] or "" %}
    {% set links = (json.loads(c[7] or "[]")) %}
    {% set edu = json.loads(c[8] or "[]") %}
    {% set exp = json.loads(c[9] or "[]") %}
    {% set skills = c[10] or "" %}



    <div class="card" id="card-{{cid}}">
      <div class="row" style="justify-content:space-between;">
        <h2 style="margin:0">{{ name or "Unnamed Candidate" }}</h2>
        <div><span class="badge">ID #{{cid}}</span></div>
      </div>

      <div class="grid section">
        <div class="col-4">
          <label>First Name</label>
          <input id="first-{{cid}}" value="{{ first }}">
        </div>
        <div class="col-4">
          <label>Middle Name</label>
          <input id="middle-{{cid}}" value="{{ middle }}">
        </div>
        <div class="col-4">
          <label>Last Name</label>
          <input id="last-{{cid}}" value="{{ last }}">
        </div>

        <div class="col-6">
          <label>Phone</label>
          <input id="phone-{{cid}}" value="{{ phone }}">
        </div>
        <div class="col-6">
          <label>Email</label>
          <input id="email-{{cid}}" value="{{ email }}">
        </div>

        <div class="col-12">
          <label>Links</label>
          <div id="links-{{cid}}">
            {% if links|length == 0 %}
            <input id="link-{{cid}}-0" value="">
            {% else %}
            {% for u in links %}
            <input id="link-{{cid}}-{{loop.index0}}" value="{{ u }}">
            {% endfor %}
            {% endif %}
          </div>
        </div>
      </div>


      <div class="section">
        <h3>Experience</h3>
        {% if exp|length == 0 %}<p class="muted">No experience parsed.</p>{% endif %}
        {% for e in exp %}
        <div class="grid" style="margin-bottom:12px;" data-exp="{{loop.index0}}">
          <div class="col-4">
            <label>Position</label>
            <input id="exp-{{cid}}-{{loop.index0}}-position" value="{{ e.get('position','') }}">
          </div>
          <div class="col-4">
            <label>Company</label>
            <input id="exp-{{cid}}-{{loop.index0}}-company_name"
              value="{{ e.get('company_name', e.get('company','')) }}">
          </div>
          <div class="col-4">
            <label>Location</label>
            <input id="exp-{{cid}}-{{loop.index0}}-location" value="{{ e.get('location','') }}">
          </div>
          <div class="col-4">
            <label>Start Date</label>
            <input id="exp-{{cid}}-{{loop.index0}}-start_date" value="{{ e.get('start_date','') }}">
          </div>
          <div class="col-4">
            <label>End Date</label>
            <input id="exp-{{cid}}-{{loop.index0}}-end_date" value="{{ e.get('end_date','') }}">
          </div>
          <div class="col-4">
            <label>Duration (months)</label>
            <input id="exp-{{cid}}-{{loop.index0}}-duration_months" value="{{ e.get('duration_months','') }}">
          </div>
          <div class="col-12">
            <label>Role Description</label>
            <textarea id="exp-{{cid}}-{{loop.index0}}-description">{{ e.get('description','') }}</textarea>
          </div>
        </div>
        {% endfor %}
      </div>

      <div class="section">
        <h3>Education</h3>
        {% set edu_list = edu if edu|length > 0 else [
        {'level':'','field':'','school_name':'','location':'','start_year':'','end_year':''} ] %}
        {% for ed in edu_list %}
        <div class="grid" style="margin-bottom:12px;" data-edu="{{loop.index0}}">
          <div class="col-4">
            <label>Level</label>
            <input id="edu-{{cid}}-{{loop.index0}}-level" value="{{ ed.get('level', ed.get('degree','')) }}">
          </div>
          <div class="col-4">
            <label>Field</label>
            <input id="edu-{{cid}}-{{loop.index0}}-field" value="{{ ed.get('field','') }}">
          </div>
          <div class="col-4">
            <label>School</label>
            <input id="edu-{{cid}}-{{loop.index0}}-school_name" value="{{ ed.get('school_name', ed.get('school','')) }}"
              readonly>
          </div>
          <div class="col-6">
            <label>Location</label>
            <input id="edu-{{cid}}-{{loop.index0}}-location" value="{{ ed.get('location','') }}">
          </div>
          <div class="col-3">
            <label>Start Year</label>
            <input id="edu-{{cid}}-{{loop.index0}}-start_year" value="{{ ed.get('start_year','') }}">
          </div>
          <div class="col-3">
            <label>End Year</label>
            <input id="edu-{{cid}}-{{loop.index0}}-end_year" value="{{ ed.get('end_year','') }}">
          </div>
        </div>
        {% endfor %}
      </div>
      <div class="grid section">
        <div class="col-12">
          <label>Skills (technical; CSV)</label>
          <textarea id="skills-{{cid}}">{{ skills }}</textarea>
        </div>
      </div>
    </div>
    {% endfor %}
    <div class="card" style="display:flex; justify-content:flex-end;">
      <button type="button" class="btn primary" id="save-all">Save All Changes</button>
    </div>
  </main>

  <script>
    document.addEventListener('DOMContentLoaded', () => {
      document.querySelectorAll('input[readonly], textarea[readonly]')
        .forEach(el => el.removeAttribute('readonly'));
      const btn = document.getElementById('save-all');
      if (btn) btn.addEventListener('click', saveAllCandidates);
    });

    function collectLinks(cid) {
      const wrap = document.getElementById(`links-${cid}`);
      const arr = [];
      if (!wrap) return arr;
      wrap.querySelectorAll('input[id^="link-"]').forEach(el => {
        const v = (el.value || "").trim();
        if (v) arr.push(v);
      });
      return arr;
    }

    function collectList(prefix, cid, attrNames) {
      const card = document.getElementById(`card-${cid}`);
      const blocks = [];
      if (!card) return blocks;
      card.querySelectorAll(`[data-${prefix}]`).forEach(block => {
        const idx = block.getAttribute(`data-${prefix}`);
        const obj = {};
        attrNames.forEach(name => {
          const el = document.getElementById(`${prefix}-${cid}-${idx}-${name}`);
          obj[name] = el ? el.value : "";
        });
        blocks.push(obj);
      });
      return blocks;
    }

    function collectCandidate(cid) {
      const first = (document.getElementById(`first-${cid}`)?.value || "");
      const middle = (document.getElementById(`middle-${cid}`)?.value || "");
      const last = (document.getElementById(`last-${cid}`)?.value || "");
      const phone = (document.getElementById(`phone-${cid}`)?.value || "");
      const email = (document.getElementById(`email-${cid}`)?.value || "");
      const links = collectLinks(cid);
      const name = [first, middle, last].filter(Boolean).join(' ').trim();

      const experience = collectList('exp', cid, [
        'position', 'company_name', 'location', 'start_date', 'end_date', 'duration_months', 'description'
      ]).map(e => {
        e.duration_months = e.duration_months ? parseInt(e.duration_months, 10) : null;
        e.skills_used_tech = e.skills_used_tech || [];
        e.skills_used_soft = e.skills_used_soft || [];
        return e;
      });

      const education = collectList('edu', cid, [
        'level', 'field', 'school_name', 'location', 'start_year', 'end_year'
      ]);

      const skills = (document.getElementById(`skills-${cid}`)?.value || "");
      const languages = "";

      return {
        name, first_name: first, middle_name: middle, last_name: last,
        phone, email, links, education, experience, skills, languages
      };
    }

    async function saveAllCandidates() {
      const btn = document.getElementById('save-all');
      if (btn) { btn.disabled = true; btn.textContent = 'Saving...'; }
      const cards = document.querySelectorAll('.card[id^="card-"]');
      let ok = 0, fail = 0;

      for (const card of cards) {
        const cid = parseInt(card.id.replace('card-', ''), 10);
        const payload = collectCandidate(cid);
        try {
          const res = await fetch(`/update/${cid}`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(payload)
          });
          if (res.ok) ok++; else fail++;
        } catch {
          fail++;
        }
      }

      if (btn) {
        btn.disabled = false;
        btn.textContent = 'Save All Changes';
      }
      alert(`Saved ${ok} candidate(s)` + (fail ? `, ${fail} failed` : ''));
    }
  </script>

</body>

</html>


// Summary: wrote 7 files, skipped 34353 files.
